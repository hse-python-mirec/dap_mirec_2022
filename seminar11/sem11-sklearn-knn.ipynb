{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема решения задачи машинного обучения (напоминание)\n",
    "\n",
    "__Вспомнить из лекции:__\n",
    "* Что такое задача машинного обучения? Что дано и что необходимо найти?\n",
    "* Какие бывают типы признаков в машинном обучении?\n",
    "* Какие бывают виды задач в машинном обучении?\n",
    "* Что такое функционал качества? Для чего он нужен?\n",
    "\n",
    "Вспомним общую схему решения задачи машинного обучения:\n",
    "\n",
    "<div>\n",
    "<img src=\"scheme.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Из исходной базы данных после предобработки мы получаем обучающую выборку $X, Y$. Матрица объекты-признаки $X$ имеет размер (число объектов) $\\times$ (число признаков). Одна строка этой матрицы соответствует одному объекту обучающей выборки, заданному как вектор длины (число признаков). Признаки - числовые характеристики объекта. Вектор правильных ответов $Y$ имеет длину (число объектов). \n",
    "\n",
    "На этапе обучения на основе обучающей выборки $X, Y$ строится (обучается) алгоритм $a(x)$. Это некая функция, которая берет на вход признаки объекта и возвращает предсказание для этого объекта: $y \\approx a(x)$. Алгоритм $a$ может делать предсказания для любых допустимых объектов; его можно применять как к обучающим объектам, так и к тем, которых алгоритм никогда не видел. В этом и состоит цель машинного обучения: выявить такие закономерности в обучающей выборке, которые позволят делать качественные (довольно точные) предсказания на новых объектах $x$. \n",
    "\n",
    "Тому, как обучать такие алгоритмы $a(x)$ по обучающей выборке, во многом ипосвящен наш курс. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрические методы. k Nearest Neighbours\n",
    "\n",
    "#### Задача 1.\n",
    "Предположим, мы решаем задачу классификации на три класса по двум признакам и используем метод k ближайших соседей с k=3 и манхэттанской метрикой. Мы имеем следующую обучающую выборку:\n",
    "\n",
    "| Признак 1 | Признак 2 | Класс |\n",
    "|-----------|-----------|-------|\n",
    "| 1         | -1        | 1     |\n",
    "| 2         | 2         | 1     |\n",
    "| 3         | 2         | 2     |\n",
    "| 1         | 0         | 3     |\n",
    "| 2         | -2        | 3     |\n",
    "\n",
    "Каковы будут предсказания для объекта $x=(2, -1)$?\n",
    "\n",
    "__Решение.__\n",
    "\n",
    "Алгоритм предсказания kNN для задачи классификации:\n",
    "1. Вычислить расстояние от каждого объекта обучающей выборки до тестового объекта.\n",
    "1. Найти k объектов обучающей выборки (соседей) с наименьшим расстоянием до тестового объекта.\n",
    "1. Вернуть наиболее встречающийся класс среди k соседей.\n",
    "\n",
    "Вычислим расстояния. Расстояние от первого объекта в обучении до тестового объекта $x$ (манхэттэнская метрика):\n",
    "\n",
    "$$|1-2| + |-1-(-1)| = 1.$$\n",
    "\n",
    "Аналогично для 2-5 объектов: получатся расстояния 3, 4, 2, 1.\n",
    "\n",
    "Находим 3 ближайших объекта: это объекты с номерами 1, 4, 5 (расстояния 1, 2, 1 соответственно). Эти три объекта относятся к классам 1, 3, 3. Чаще всего встречается класс 3, поэтому предсказываем 3.\n",
    "\n",
    "#### Задача 2.\n",
    "Визуализируйте разделяющую поверхность между классами для следующей выборки:\n",
    "\n",
    "| Признак 1 | Признак 2 | Класс |\n",
    "|-----------|-----------|-------|\n",
    "| 2         | 2        | 1     |\n",
    "| 3         | 2         | 1     |\n",
    "| 2         | 0         | 2     |\n",
    "| 1         | -1         | 3     |\n",
    "| 1        | 1        | 3     |\n",
    "\n",
    "Используйте k=1 и евклидово расстояние.\n",
    "\n",
    "__Решение.__\n",
    "\n",
    "В задачах классификации с двумя признаками мы можем изобразить признаковое пространство на плоскости и раскрасить его в разные цвета в соответствии с классом каждой точки плоскости. В этом и состоит сейчас наша задача.\n",
    "\n",
    "Для начала отобразим на плоскости обучающую выборку - пять точек - в соответствии с их координатами.\n",
    "\n",
    "При $k=1$ каждая точка плоскости будет относиться к тому же классу, что и ближайший к ней объект обучающей выборки. Если нам даны две точки разных классов, то чтобы провести между ними границу классов, нужно построить серединный перпендикудяр. Для случая с несколькими точками нужно построить несколько серединных перпендикуляров, найти их точки пересечения и определить, какие области к каким классам относятся. Более строго такая конструкция задается с помощью [Диаграммы Вороного](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%B0%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D0%BE%D0%B3%D0%BE), но мы не будем вдаваться в ее детали.\n",
    "\n",
    "<div>\n",
    "<img src=\"classifi.png\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "#### Задача 3.\n",
    "Предположим, мы решаем задачу регрессии по двум признакам и используем метод k ближайших соседей с k=3 и манхэттанской метрикой. Мы имеем следующую обучающую выборку:\n",
    "\n",
    "| Признак 1 | Признак 2 | Ответ |\n",
    "|-----------|-----------|-------|\n",
    "| 1         | -1        | 3.5     |\n",
    "| 2         | 2         | 2.3     |\n",
    "| 3         | 2         | 1.7     |\n",
    "| 1         | 0         | -0.4     |\n",
    "| 2         | -2        | 0.1     |\n",
    "\n",
    "Каковы будут предсказания для объекта $x=(2, -1)$?\n",
    "\n",
    "__Решение.__\n",
    "Предсказания kNN для регрессии отличаются от предсказаний для классификации только финальным шагом: вместо поиска наиболее часто встречающегося класса нужно усреднить ответы на соседях. Признаки в этой задаче те же, что в задаче 1, поэтому соседей мы уже знаем: это объекты с номерами 1, 4, 5. На них мы имеем ответы 3.5, -0.3, 0.1. Усредним их: (3.5-0.4+0.1)/3 = 1.1. Предсказываем 1.1.\n",
    "\n",
    "#### Вопрос: каковы параметры и гиперпараметры метода kNN?\n",
    "__Ответ:__\n",
    "\n",
    "Параметры - это величины, которые мы настраиваем в процессе обучения по обучающей выборке. В методе kNN нет как такового обучения - это очень простой эвристический алгоритм. Под параметрами в kNN можно понимать обучающую выборку. В другой трактовке у метода нет параметров.\n",
    "\n",
    "Гиперпараметры - это величины, которые мы должны установить до начала обучения модели. Гиперпараметры не настраиваются по обучающей выборке в процессе обучения модели. Два самых важных гиперпараметры метода kNN - это число соседей k и метрика. Используя разные комбинации этих гиперпараметров, можно получать совершенно разное качество работы алгоритма. Гиперпараметры обычно настраивают по валидационной выборке или используя кросс-валидацию.\n",
    "\n",
    "#### Какова динамика качества работы kNN при увеличении k?\n",
    "\n",
    "__Ответ:__\n",
    "\n",
    "При $k=1$ вокруг каждого объекта обучающей выборки создается область его класса. Если, к примеру, в \"большую\" область одного класса случайно попал один шумовой объект другого класса, вокруг этого шумового объекта будет \"остров\" предсказания другого класса. Это нелогично и говорит о переобучении.\n",
    "\n",
    "При $k$, равном числу объектов в выборке, для всех объектов будет предсказываться одно и то же, что вновь говорит о низком качестве работы классификатора. Получается, что качество kNN при увеличении $k$ должно сначала расти, а потом падать, и оптимум будем где-то посередине.\n",
    "\n",
    "Рассмотрим синтетический пример: на рисунке визуализирована обучающая выборка (\"настоящая\" разделяющая поверхность - прямая) и разделяющая поверхность kNN по аналогии с задачей 2, и на разных графиках используется разное число соседей $k$:\n",
    "\n",
    "<div>\n",
    "<img src=\"k_grid.png\" width=\"550\"/>\n",
    "</div>\n",
    "\n",
    "При использовании малых $k$ разделяющая поверхность слишком сложная, на нее оказывают сильное воздействие шумовые объекты. Далее поверхность становится все ровнее и ровнее и при $k=50$ выглядит наиболее разумно. При большем k разделяющая поверхность уходит от линейной, и оранжевый класс \"захватывает\" синий.\n",
    "\n",
    "#### Почему при использовании kNN важно нормировать данные?\n",
    "\n",
    "__Ответ:__\n",
    "\n",
    "Рассмотрим для примера манхэттэнскую метрику. Если один признак будет иметь масштаб около 1000, а другой - около 1, то когда мы будем складывать модули разностей для этих двух признаков, второй признак практически не будет иметь влияния на ответ. Если же признаки отнормировать, но они все будут в одной шкале.\n",
    "\n",
    "### Практическая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# изображения цифр\n",
    "from sklearn.datasets import load_digits\n",
    "# классификатор\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# шаффлер данных\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "data = load_digits()\n",
    "X = data.images\n",
    "y = data.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1797, 64),\n",
      "Target shape: (1797,)\n",
      "Target samples: [6 1 0 6 9 5 0 9 8 1]\n"
     ]
    }
   ],
   "source": [
    "# вытягиваем квадратное изображение в вектор, чтобы получить матрицу объекты-признаки\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# перемешиваем данные\n",
    "X, y = shuffle(X, y)\n",
    "print(f\"Features shape: {X.shape},\\nTarget shape: {y.shape}\")\n",
    "print(f\"Target samples: {y[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:700, :], y[:700]\n",
    "X_val, y_val = X[700:1300, :], y[700:1300]\n",
    "X_test, y_test = X[1300:, :], y[1300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем классификатор и делаем предсказания\n",
    "clf.fit(X_train, y_train)\n",
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.9778672032193159\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем простейшую метрику качества алгоритма - долю правильных ответов\n",
    "print(\"Accuracy is: \", np.mean(y_test==y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая, что у нас 10 классов, то вероятность угадать правильный ответ много раз очень мала. Поэтому полученное значение accuracy - очень хороший результат!\n",
    "\n",
    "Попробуем использовать разные значения гиперпараметра k. Сравнивать разные значения k по обучающей выборке бесполезно: каждый объект является ближайшим сам к себе и оптимальное k будет равно 1. Будем сравнивать разные k по качеству на валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1; accuracy = 0.982\n",
      "k = 2; accuracy = 0.968\n",
      "k = 3; accuracy = 0.973\n",
      "k = 4; accuracy = 0.973\n",
      "k = 5; accuracy = 0.967\n",
      "k = 6; accuracy = 0.967\n",
      "k = 7; accuracy = 0.962\n",
      "k = 8; accuracy = 0.960\n",
      "k = 9; accuracy = 0.958\n",
      "k = 10; accuracy = 0.958\n",
      "k = 11; accuracy = 0.958\n",
      "k = 12; accuracy = 0.957\n",
      "k = 13; accuracy = 0.955\n",
      "k = 14; accuracy = 0.955\n",
      "k = 15; accuracy = 0.952\n",
      "k = 16; accuracy = 0.947\n",
      "k = 17; accuracy = 0.947\n",
      "k = 18; accuracy = 0.938\n",
      "k = 19; accuracy = 0.940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подбор k на валидационной выборке:\n",
    "k_best = -1\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 20):\n",
    "    y_predicted = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train).predict(X_val)\n",
    "\n",
    "    val_accuracy = np.mean(y_predicted==y_val)                      \n",
    "    print(f\"k = {k}; accuracy = {val_accuracy:.3f}\")\n",
    "                           \n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        k_best = k\n",
    "\n",
    "k_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним accuracy на обучении, валидации и тесте для найденного лучшего значения k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Accuracy: 0.982\n",
      "Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=k_best)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "for X_data, y_data in zip([X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    y_predicted = clf.predict(X_data)\n",
    "    print(f\"Accuracy: {np.mean(y_predicted==y_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на обучающей выборке самое лучшее, но оно обманчиво, ведь алгоритм уже знает эти объекты.\n",
    "На валидационной выборке мы тоже использовали ответы - уже для подбора гиперпараметра k, так что этот показатель тоже не совсем честный. \n",
    "И действительно, качество на тестовой выборке (ответы для которой мы нигде не подсматривали) может оказаться хуже, чем на валидационной выборке.\n",
    "\n",
    "_Вывод_: оценивать качество алгоритма нужно на отложенной выборке, которая не используется нигде в обучении и не используется в подборе гиперпараметров."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
